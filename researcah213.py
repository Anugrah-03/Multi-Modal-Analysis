{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":6120,"sourceType":"modelInstanceVersion","modelInstanceId":4603,"modelId":2797},{"sourceId":6125,"sourceType":"modelInstanceVersion","modelInstanceId":4596,"modelId":2797},{"sourceId":6127,"sourceType":"modelInstanceVersion","modelInstanceId":4598,"modelId":2797}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":602.064687,"end_time":"2024-08-24T00:33:30.797234","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-08-24T00:23:28.732547","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1487ca0f12e54416813264d376a5bca7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2947551bcf0a408186aae67f6f2747e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db3e0eb39e814af693359957fbab5580","IPY_MODEL_5321a688cc274e38af90fbf1a691d531","IPY_MODEL_756f9426fcc04add9c6914edf427c2cb"],"layout":"IPY_MODEL_31141aa3a0124c0ba42d1e48f2648497"}},"2cb8ef13decc4d95addfcf74ee23f5aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31141aa3a0124c0ba42d1e48f2648497":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36063ac8dada4464b70ffbfbfc58913b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb8ef13decc4d95addfcf74ee23f5aa","max":1266,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d118563083b84225b3ed7a1ff792ba99","value":1266}},"4520fc00683b4a8f85b0f26c1aa28ddb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4584f9bddd5941119aef43c0eeda38e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dbfb59502ba47e4bc75379f767ea1f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9dee742323a94990b37a5f0d57d840ff","IPY_MODEL_790c478401f847b29685e10b9c046249","IPY_MODEL_cb1d363b494e48ba9e327c452d5ade48"],"layout":"IPY_MODEL_c1e2f87bdd474ae6ae78113cab3c83e2"}},"5321a688cc274e38af90fbf1a691d531":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c6128011d7f4cb18dfeffafaea29492","max":4706,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e3c60bbf72241fd87644c619893183d","value":4706}},"60f300155bd346ae81eb32e7087ede2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6908d58f86004f188b8bf5b6a285eb01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1f321c26cf49dda200209b99336f90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"756f9426fcc04add9c6914edf427c2cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4584f9bddd5941119aef43c0eeda38e2","placeholder":"â€‹","style":"IPY_MODEL_cdfc717020e341a0b8a70b9f5b608c3f","value":"â€‡4706/4706â€‡[00:19&lt;00:00,â€‡302.99it/s]"}},"790c478401f847b29685e10b9c046249":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6908d58f86004f188b8bf5b6a285eb01","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b1f321c26cf49dda200209b99336f90","value":3}},"8e3c60bbf72241fd87644c619893183d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fda110e09a84dd4b88e6197eba1988c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"982090bddcb94763afb17af5da403ccf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c6128011d7f4cb18dfeffafaea29492":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dee742323a94990b37a5f0d57d840ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4520fc00683b4a8f85b0f26c1aa28ddb","placeholder":"â€‹","style":"IPY_MODEL_b7af3470a2ac47a0b51ac9670bee269e","value":"Loadingâ€‡Imagesâ€‡:â€‡100%"}},"9f262140e7d543ab90f61a0f239c288b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9cf9946204d426eaf1c11793d5f65cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9dec529fbe142e4bf4881e2e73a0fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4cfcc31d589452d94662b08cb7e3dc3","IPY_MODEL_36063ac8dada4464b70ffbfbfc58913b","IPY_MODEL_b0dc0ae740374a31953b3dee0a3ef173"],"layout":"IPY_MODEL_60f300155bd346ae81eb32e7087ede2b"}},"b0dc0ae740374a31953b3dee0a3ef173":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1487ca0f12e54416813264d376a5bca7","placeholder":"â€‹","style":"IPY_MODEL_d2d9168680094a63969bc43a9fab5d81","value":"â€‡1266/1266â€‡[00:04&lt;00:00,â€‡319.02it/s]"}},"b7af3470a2ac47a0b51ac9670bee269e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1e2f87bdd474ae6ae78113cab3c83e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4cfcc31d589452d94662b08cb7e3dc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f262140e7d543ab90f61a0f239c288b","placeholder":"â€‹","style":"IPY_MODEL_8fda110e09a84dd4b88e6197eba1988c","value":"Loadingâ€‡Imagesâ€‡:â€‡100%"}},"cb1d363b494e48ba9e327c452d5ade48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e31a94f2f53e48889936e811bd87b4e5","placeholder":"â€‹","style":"IPY_MODEL_db221720acdd4225831461447475982d","value":"â€‡3/3â€‡[00:00&lt;00:00,â€‡138.00it/s]"}},"cdfc717020e341a0b8a70b9f5b608c3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d118563083b84225b3ed7a1ff792ba99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2d9168680094a63969bc43a9fab5d81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db221720acdd4225831461447475982d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db3e0eb39e814af693359957fbab5580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982090bddcb94763afb17af5da403ccf","placeholder":"â€‹","style":"IPY_MODEL_a9cf9946204d426eaf1c11793d5f65cb","value":"Loadingâ€‡Imagesâ€‡:â€‡100%"}},"e31a94f2f53e48889936e811bd87b4e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"papermill\":{\"duration\":0.017796,\"end_time\":\"2024-08-24T00:23:31.644148\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:31.626352\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ISIC 2024 - Skin Lesion Detection with 3D-TB using KerasCV and Keras\n# \n# > The goal of this competition is to detect skin cancers in lesions cropped from 3D total body photographs.\n# \n# <div align=\"center\">\n#   <image src=\"https://i.ibb.co/PYLN6QR/isic2024.jpg\">\n# </div>\n# \n# This notebook guides through training and deploying a Deep Learning model for skin cancer detection using skin lesion data from 3D total body photographs. Specifically, we'll employ the EfficientNetV2 backbone from KerasCV on the competition dataset. The notebook integrates both image data and tabular features (e.g., age, sex) to enhance skin cancer detection.\n# \n# **Fun fact:** This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. Leveraging KerasCV and Keras allows flexibility in choosing the preferred backend. Explore more details on [Keras](https://keras.io/keras_core/announcement/).\n# \n# In this notebook, following lessions will be covered:\n# \n# - Designing a data pipeline for a multi-input model.\n# - Creating a random augmentation pipeline with KerasCV.\n# - Efficiently loading data using [`tf.data`](https://www.tensorflow.org/guide/data).\n# - Utilizing KerasCV presets to build the model.\n# - Training the model.\n# - Performing inference and generating submissions on testing data.\n# \n# **Note:** For a deeper understanding of KerasCV, refer to the [KerasCV guides](https://keras.io/guides/keras_cv/).\n\n# %% [markdown] {\"papermill\":{\"duration\":0.016228,\"end_time\":\"2024-08-24T00:23:31.677001\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:31.660773\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ“š | Import Libraries\n\n# %% [code] {\"papermill\":{\"duration\":18.334972,\"end_time\":\"2024-08-24T00:23:50.028906\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:31.693934\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:16.418113Z\",\"iopub.execute_input\":\"2024-12-30T07:22:16.418439Z\",\"iopub.status.idle\":\"2024-12-30T07:22:32.736462Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:16.418398Z\",\"shell.execute_reply\":\"2024-12-30T07:22:32.735729Z\"}}\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # other options: tensorflow or torch\n\nimport keras_cv\nimport keras\nfrom keras import ops\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport joblib\n\nimport matplotlib.pyplot as plt\n\n# %% [markdown] {\"papermill\":{\"duration\":0.017136,\"end_time\":\"2024-08-24T00:23:50.063500\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.046364\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Library Versions\n\n# %% [code] {\"papermill\":{\"duration\":0.026379,\"end_time\":\"2024-08-24T00:23:50.106978\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.080599\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:32.738027Z\",\"iopub.execute_input\":\"2024-12-30T07:22:32.738642Z\",\"iopub.status.idle\":\"2024-12-30T07:22:32.744218Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:32.738603Z\",\"shell.execute_reply\":\"2024-12-30T07:22:32.743214Z\"}}\nprint(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasCV:\", keras_cv.__version__)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.01684,\"end_time\":\"2024-08-24T00:23:50.141029\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.124189\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # âš™ï¸ | Configuration\n\n# %% [code] {\"papermill\":{\"duration\":0.025831,\"end_time\":\"2024-08-24T00:23:50.183771\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.157940\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:32.745450Z\",\"iopub.execute_input\":\"2024-12-30T07:22:32.745703Z\",\"iopub.status.idle\":\"2024-12-30T07:22:32.838833Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:32.745679Z\",\"shell.execute_reply\":\"2024-12-30T07:22:32.837950Z\"}}\nclass CFG:\n    verbose = 1  # Verbosity\n    seed = 42  # Random seed\n    neg_sample = 0.01 # Downsample negative calss\n    pos_sample = 5.0  # Upsample positive class\n    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n    image_size = [128, 128]  # Input image size\n    epochs = 8 # Training epochs\n    batch_size = 16  # Batch size\n    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n    class_names = ['target']\n    num_classes = 1\n\n# %% [markdown] {\"papermill\":{\"duration\":0.016545,\"end_time\":\"2024-08-24T00:23:50.217205\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.200660\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # â™»ï¸ | Reproducibility \n# Sets value for random seed to produce similar result in each run.\n\n# %% [code] {\"papermill\":{\"duration\":0.025357,\"end_time\":\"2024-08-24T00:23:50.259691\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.234334\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:32.841109Z\",\"iopub.execute_input\":\"2024-12-30T07:22:32.841747Z\",\"iopub.status.idle\":\"2024-12-30T07:22:32.849918Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:32.841706Z\",\"shell.execute_reply\":\"2024-12-30T07:22:32.849200Z\"}}\nkeras.utils.set_random_seed(CFG.seed)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.016669,\"end_time\":\"2024-08-24T00:23:50.293364\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.276695\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ“ | Dataset Path\n\n# %% [code] {\"papermill\":{\"duration\":0.025171,\"end_time\":\"2024-08-24T00:23:50.348662\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.323491\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:32.850755Z\",\"iopub.execute_input\":\"2024-12-30T07:22:32.851020Z\",\"iopub.status.idle\":\"2024-12-30T07:22:32.860676Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:32.850996Z\",\"shell.execute_reply\":\"2024-12-30T07:22:32.859848Z\"}}\nBASE_PATH = \"/kaggle/input/isic-2024-challenge\"\n\n# %% [markdown] {\"papermill\":{\"duration\":0.017251,\"end_time\":\"2024-08-24T00:23:50.383241\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.365990\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ“– | Meta Data\n# \n# In this dataset, following information is available:\n# \n# - **train-image/**: Contains image files for the training set (provided for train only).\n# - **train-image.hdf5**: Training image data stored in a single HDF5 file, where each image is indexed by its `isic_id`.\n# - **train-metadata.csv**: Metadata corresponding to the training set, including:\n#   - `isic_id`: Unique image ID used to query images in the HDF5 file.\n#   - `patient_id`: Unique patient ID.\n#   - `sex`: Gender of the patient.\n#   - `age_approx`: Approximate age of the patient.\n#   - `anatom_site_general`: Location of the lesion.\n#   - Other relevant metadata fields.\n# - **test-image.hdf5**: Testing image data stored in a single HDF5 file, initially containing 3 testing examples to validate the inference pipeline. Upon notebook submission, this file is replaced with the full hidden testing set, which contains approximately 500,000 images.\n# - **test-metadata.csv**: Metadata corresponding to the testing subset.\n\n# %% [code] {\"papermill\":{\"duration\":9.56639,\"end_time\":\"2024-08-24T00:23:59.966363\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:50.399973\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:32.861839Z\",\"iopub.execute_input\":\"2024-12-30T07:22:32.862417Z\",\"iopub.status.idle\":\"2024-12-30T07:22:41.529659Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:32.862380Z\",\"shell.execute_reply\":\"2024-12-30T07:22:41.528757Z\"}}\n# Train + Valid\ndf = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\ndf = df.ffill()\ndisplay(df.head(2))\n\n# Testing\ntesting_df = pd.read_csv(f'{BASE_PATH}/test-metadata.csv')\ntesting_df = testing_df.ffill()\ndisplay(testing_df.head(2))\n\n# %% [markdown] {\"papermill\":{\"duration\":0.01743,\"end_time\":\"2024-08-24T00:24:00.003338\",\"exception\":false,\"start_time\":\"2024-08-24T00:23:59.985908\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # âš–ï¸ | Handle Class Imbalance\n\n# %% [markdown] {\"papermill\":{\"duration\":0.017177,\"end_time\":\"2024-08-24T00:24:00.038195\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:00.021018\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Sample Data\n# \n# There is a significant class imbalance in the dataset, with a large number of negative samples compared to positive samples. To address this issue, the negative class will be downsampled and upsample the positive class. To experiment with full dataset, simply adjust the `pos_sample` and `neg_sample` settings in `CFG`.\n\n# %% [code] {\"_kg_hide-input\":true,\"papermill\":{\"duration\":0.291517,\"end_time\":\"2024-08-24T00:24:00.347304\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:00.055787\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:41.530575Z\",\"iopub.execute_input\":\"2024-12-30T07:22:41.530854Z\",\"iopub.status.idle\":\"2024-12-30T07:22:41.753551Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:41.530815Z\",\"shell.execute_reply\":\"2024-12-30T07:22:41.752559Z\"}}\nprint(\"Class Distribution Before Sampling (%):\")\ndisplay(df.target.value_counts(normalize=True)*100)\n\n# Sampling\npositive_df = df.query(\"target==0\").sample(frac=CFG.neg_sample, random_state=CFG.seed)\nnegative_df = df.query(\"target==1\").sample(frac=CFG.pos_sample, replace=True, random_state=CFG.seed)\ndf = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0)\n\nprint(\"\\nCalss Distribution After Sampling (%):\")\ndisplay(df.target.value_counts(normalize=True)*100)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.017564,\"end_time\":\"2024-08-24T00:24:00.384467\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:00.366903\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Class Weight\n# \n# Even after downsampling the negative class and upsampling the positive class, there remains a significant class imbalance. To further address this imbalance during training, loss weighting will be used. This technique ensures that the model weights are updated more heavily for the positive samples, thereby reducing the bias towards the negative class. The following code computes the class weights for the loss:\n\n# %% [code] {\"papermill\":{\"duration\":1.022261,\"end_time\":\"2024-08-24T00:24:01.424529\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:00.402268\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:41.757118Z\",\"iopub.execute_input\":\"2024-12-30T07:22:41.757357Z\",\"iopub.status.idle\":\"2024-12-30T07:22:42.296431Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:41.757332Z\",\"shell.execute_reply\":\"2024-12-30T07:22:42.295478Z\"}}\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Assume df is your DataFrame and 'target' is the column with class labels\nclass_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\nclass_weights = dict(enumerate(class_weights))\nprint(\"Class Weights:\", class_weights)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.017499,\"end_time\":\"2024-08-24T00:24:01.460041\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:01.442542\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ–¼ï¸ | Load Image Byte String\n# \n# In this competition, images are provided as byte strings. The following code snippet demonstrates how to load these images into memory. One might wonder why the provided `jpeg` images aren't being used in the `/train-image` folder for training. This is because testing images are not provided as JPEG images; instead, they are provided as byte strings. Why use byte strings? They occupy significantly less memory compared to `np.array` representations.\n\n# %% [code] {\"papermill\":{\"duration\":0.036408,\"end_time\":\"2024-08-24T00:24:01.514455\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:01.478047\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:42.298508Z\",\"iopub.execute_input\":\"2024-12-30T07:22:42.299088Z\",\"iopub.status.idle\":\"2024-12-30T07:22:42.321550Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:42.299059Z\",\"shell.execute_reply\":\"2024-12-30T07:22:42.320797Z\"}}\nimport h5py\n\ntraining_validation_hdf5 = h5py.File(f\"{BASE_PATH}/train-image.hdf5\", 'r')\ntesting_hdf5 = h5py.File(f\"{BASE_PATH}/test-image.hdf5\", 'r')\n\n# %% [markdown] {\"papermill\":{\"duration\":0.01778,\"end_time\":\"2024-08-24T00:24:01.550380\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:01.532600\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Check Image\n# \n# Examining a sample image from the provided data is important. This step allows for a closer inspection of the image quality and content, ensuring it meets the requirements for further processing.\n\n# %% [code] {\"papermill\":{\"duration\":0.507857,\"end_time\":\"2024-08-24T00:24:02.076049\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:01.568192\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:22:42.322606Z\",\"iopub.execute_input\":\"2024-12-30T07:22:42.323001Z\",\"iopub.status.idle\":\"2024-12-30T07:22:42.761677Z\",\"shell.execute_reply.started\":\"2024-12-30T07:22:42.322973Z\",\"shell.execute_reply\":\"2024-12-30T07:22:42.760728Z\"}}\nisic_id = df.isic_id.iloc[0]\n\n# Image as Byte String\nbyte_string = training_validation_hdf5[isic_id][()]\nprint(f\"Byte String: {byte_string[:20]}....\")\n\n# Convert byte string to numpy array\nnparr = np.frombuffer(byte_string, np.uint8)\n\nprint(\"Image:\")\nimage = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1] # reverse last axis for bgr -> rgb\nplt.imshow(image);\n\n# %% [markdown] {\"execution\":{\"iopub.execute_input\":\"2024-06-30T05:46:56.020823Z\",\"iopub.status.busy\":\"2024-06-30T05:46:56.019942Z\",\"iopub.status.idle\":\"2024-06-30T05:46:56.844037Z\",\"shell.execute_reply\":\"2024-06-30T05:46:56.843042Z\",\"shell.execute_reply.started\":\"2024-06-30T05:46:56.020788Z\"},\"papermill\":{\"duration\":0.019778,\"end_time\":\"2024-08-24T00:24:02.116523\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.096745\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ”ª | Data Split\n# \n# In the following code,the data will be splitted into `5` stratified folds and use the first fold for training and validation. It's important to note that `StratifiedGroupKFold` is being used to ensure that `patient_id`s do not overlap between the training and validation datasets. This prevents data leakage, where the model could potentially peak at data it should not have access to.\n# \n# > **Note**: Data leakage can lead to artificially high validation scores that do not reflect real-world performance.\n\n# %% [code] {\"papermill\":{\"duration\":0.533119,\"end_time\":\"2024-08-24T00:24:02.669572\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.136453\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:23:20.114472Z\",\"iopub.execute_input\":\"2024-12-30T07:23:20.115198Z\",\"iopub.status.idle\":\"2024-12-30T07:23:20.515533Z\",\"shell.execute_reply.started\":\"2024-12-30T07:23:20.115164Z\",\"shell.execute_reply\":\"2024-12-30T07:23:20.514628Z\"}}\nfrom sklearn.model_selection import StratifiedGroupKFold\n\ndf = df.reset_index(drop=True) # ensure continuous index\ndf[\"fold\"] = -1\nsgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\nfor i, (training_idx, validation_idx) in enumerate(sgkf.split(df, y=df.target, groups=df.patient_id)):\n    df.loc[validation_idx, \"fold\"] = int(i)\n\n# Use first fold for training and validation\ntraining_df = df.query(\"fold!=0\")\nvalidation_df = df.query(\"fold==0\")\nprint(f\"# Num Train: {len(training_df)} | Num Valid: {len(validation_df)}\")\n\n# %% [markdown] {\"papermill\":{\"duration\":0.019792,\"end_time\":\"2024-08-24T00:24:02.710591\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.690799\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Class Distribution in Training\n\n# %% [code] {\"papermill\":{\"duration\":0.032588,\"end_time\":\"2024-08-24T00:24:02.763181\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.730593\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:23:22.138720Z\",\"iopub.execute_input\":\"2024-12-30T07:23:22.139435Z\",\"iopub.status.idle\":\"2024-12-30T07:23:22.146922Z\",\"shell.execute_reply.started\":\"2024-12-30T07:23:22.139402Z\",\"shell.execute_reply\":\"2024-12-30T07:23:22.146112Z\"}}\ntraining_df.target.value_counts()\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020047,\"end_time\":\"2024-08-24T00:24:02.803749\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.783702\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Class Distribution in Validation\n\n# %% [code] {\"papermill\":{\"duration\":0.031501,\"end_time\":\"2024-08-24T00:24:02.855751\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.824250\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:23:24.343499Z\",\"iopub.execute_input\":\"2024-12-30T07:23:24.343844Z\",\"iopub.status.idle\":\"2024-12-30T07:23:24.350841Z\",\"shell.execute_reply.started\":\"2024-12-30T07:23:24.343813Z\",\"shell.execute_reply\":\"2024-12-30T07:23:24.349811Z\"}}\nvalidation_df.target.value_counts()\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020298,\"end_time\":\"2024-08-24T00:24:02.897461\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.877163\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ“Š | Tabular Features\n# \n# In this competition, alongside image data, tabular features such as age, sex, and the location of the lesion are available. Previous competitions, like [ISIC 2020](https://www.kaggle.com/c/siim-isic-melanoma-classification/overview), have demonstrated that incorporating these tabular features can significantly enhance model performance. A similar improvement is anticipated here.\n# \n# The following code snippet provides a method for selecting which tabular features to include. It is encouraged to experiment with various combinations to determine the most effective set.\n\n# %% [code] {\"papermill\":{\"duration\":0.028923,\"end_time\":\"2024-08-24T00:24:02.946707\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.917784\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:23:26.331173Z\",\"iopub.execute_input\":\"2024-12-30T07:23:26.331776Z\",\"iopub.status.idle\":\"2024-12-30T07:23:26.336088Z\",\"shell.execute_reply.started\":\"2024-12-30T07:23:26.331744Z\",\"shell.execute_reply\":\"2024-12-30T07:23:26.335175Z\"}}\n# Categorical features which will be one hot encoded\nCATEGORICAL_COLUMNS = [\"sex\", \"anatom_site_general\",\n            \"tbp_tile_type\",\"tbp_lv_location\", ]\n\n# Numeraical features which will be normalized\nNUMERIC_COLUMNS = [\"age_approx\", \"tbp_lv_nevi_confidence\", \"clin_size_long_diam_mm\",\n           \"tbp_lv_areaMM2\", \"tbp_lv_area_perim_ratio\", \"tbp_lv_color_std_mean\",\n           \"tbp_lv_deltaLBnorm\", \"tbp_lv_minorAxisMM\", ]\n\n# Tabular feature columns\nFEAT_COLS = CATEGORICAL_COLUMNS + NUMERIC_COLUMNS\n\n# %% [markdown] {\"papermill\":{\"duration\":0.02017,\"end_time\":\"2024-08-24T00:24:02.987306\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:02.967136\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸš | DataLoader\n# \n# This DataLoader is designed to process both `images` and tabular `features` simultaneously as inputs. It applies augmentations like `flip` and `cutout`, with additional options available such as random brightness, contrast, zoom, and rotation. Experimentation with different augmentations is encouraged. More details on the available augmentations in KerasCV can be found [here](https://keras.io/api/keras_cv/layers/preprocessing/).\n# \n# > Note: Unlike standard augmentations, these augmentations are applied to a batch, enhancing training speed and reducing CPU bottlenecks.\n\n# %% [code] {\"papermill\":{\"duration\":0.042596,\"end_time\":\"2024-08-24T00:24:03.050235\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:03.007639\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:23:28.459717Z\",\"iopub.execute_input\":\"2024-12-30T07:23:28.460531Z\",\"iopub.status.idle\":\"2024-12-30T07:23:28.471873Z\",\"shell.execute_reply.started\":\"2024-12-30T07:23:28.460496Z\",\"shell.execute_reply\":\"2024-12-30T07:23:28.471076Z\"}}\ndef build_augmenter():\n    # Define augmentations\n    aug_layers = [\n        keras_cv.layers.RandomCutout(height_factor=(0.02, 0.06), width_factor=(0.02, 0.06)),\n        keras_cv.layers.RandomFlip(mode=\"horizontal\"),\n    ]\n    \n    # Apply augmentations to random samples\n    aug_layers = [keras_cv.layers.RandomApply(x, rate=0.5) for x in aug_layers]\n    \n    # Build augmentation layer\n    augmenter = keras_cv.layers.Augmenter(aug_layers)\n\n    # Apply augmentations\n    def augment(inp, label):\n        images = inp[\"images\"]\n        aug_data = {\"images\": images}\n        aug_data = augmenter(aug_data)\n        inp[\"images\"] = aug_data[\"images\"]\n        return inp, label\n    return augment\n\n\ndef build_decoder(with_labels=True, target_size=CFG.image_size):\n    def decode_image(inp):\n        # Read jpeg image\n        file_bytes = inp[\"images\"]\n        image = tf.io.decode_jpeg(file_bytes)\n        \n        # Resize\n        image = tf.image.resize(image, size=target_size, method=\"area\")\n        \n        # Rescale image\n        image = tf.cast(image, tf.float32)\n        image /= 255.0\n        \n        # Reshape\n        image = tf.reshape(image, [*target_size, 3])\n        \n        inp[\"images\"] = image\n        return inp\n\n    def decode_label(label, num_classes):\n        label = tf.cast(label, tf.float32)\n        label = tf.reshape(label, [num_classes])\n        return label\n\n    def decode_with_labels(inp, label=None):\n        inp = decode_image(inp)\n        label = decode_label(label, CFG.num_classes)\n        return (inp, label)\n\n    return decode_with_labels if with_labels else decode_image\n\n\ndef build_dataset(\n    isic_ids,\n    hdf5,\n    features,\n    labels=None,\n    batch_size=32,\n    decode_fn=None,\n    augment_fn=None,\n    augment=False,\n    shuffle=1024,\n    cache=True,\n    drop_remainder=False,\n):\n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n\n    if augment_fn is None:\n        augment_fn = build_augmenter()\n\n    AUTO = tf.data.experimental.AUTOTUNE\n\n    images = [None]*len(isic_ids)\n    for i, isic_id in enumerate(tqdm(isic_ids, desc=\"Loading Images \")):\n        images[i] = hdf5[isic_id][()]\n        \n    inp = {\"images\": images, \"features\": features}\n    slices = (inp, labels) if labels is not None else inp\n\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.cache() if cache else ds\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    if shuffle:\n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt = tf.data.Options()\n        opt.deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.prefetch(AUTO)\n    return ds\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020252,\"end_time\":\"2024-08-24T00:24:03.091138\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:03.070886\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Build Training & Validation Dataset\n# \n# In the following code, **training** and **validation** data loaders will be created.\n\n# %% [code] {\"papermill\":{\"duration\":27.294922,\"end_time\":\"2024-08-24T00:24:30.406449\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:03.111527\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:23:31.819419Z\",\"iopub.execute_input\":\"2024-12-30T07:23:31.819757Z\",\"iopub.status.idle\":\"2024-12-30T07:24:03.997366Z\",\"shell.execute_reply.started\":\"2024-12-30T07:23:31.819728Z\",\"shell.execute_reply\":\"2024-12-30T07:24:03.996677Z\"}}\n## Train\nprint(\"# Training:\")\ntraining_features = dict(training_df[FEAT_COLS])\ntraining_ids = training_df.isic_id.values\ntraining_labels = training_df.target.values\ntraining_ds = build_dataset(training_ids, training_validation_hdf5, training_features, \n                         training_labels, batch_size=CFG.batch_size,\n                         shuffle=True, augment=True)\n\n# Valid\nprint(\"# Validation:\")\nvalidation_features = dict(validation_df[FEAT_COLS])\nvalidation_ids = validation_df.isic_id.values\nvalidation_labels = validation_df.target.values\nvalidation_ds = build_dataset(validation_ids, training_validation_hdf5, validation_features,\n                         validation_labels, batch_size=CFG.batch_size,\n                         shuffle=False, augment=False)\n\n# %% [code] {\"papermill\":{\"duration\":0.082237,\"end_time\":\"2024-08-24T00:24:30.557510\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:30.475273\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:24:03.999134Z\",\"iopub.execute_input\":\"2024-12-30T07:24:03.999384Z\",\"iopub.status.idle\":\"2024-12-30T07:24:04.028306Z\",\"shell.execute_reply.started\":\"2024-12-30T07:24:03.999360Z\",\"shell.execute_reply\":\"2024-12-30T07:24:04.027469Z\"}}\nfeature_space = keras.utils.FeatureSpace(\n    features={\n        # Categorical features encoded as integers\n        \"sex\": \"string_categorical\",\n        \"anatom_site_general\": \"string_categorical\",\n        \"tbp_tile_type\": \"string_categorical\",\n        \"tbp_lv_location\": \"string_categorical\",\n        # Numerical features to discretize\n        \"age_approx\": \"float_discretized\",\n        # Numerical features to normalize\n        \"tbp_lv_nevi_confidence\": \"float_normalized\",\n        \"clin_size_long_diam_mm\": \"float_normalized\",\n        \"tbp_lv_areaMM2\": \"float_normalized\",\n        \"tbp_lv_area_perim_ratio\": \"float_normalized\",\n        \"tbp_lv_color_std_mean\": \"float_normalized\",\n        \"tbp_lv_deltaLBnorm\": \"float_normalized\",\n        \"tbp_lv_minorAxisMM\": \"float_normalized\",\n    },\n    output_mode=\"concat\",\n)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.028887,\"end_time\":\"2024-08-24T00:24:30.612429\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:30.583542\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Configuring a `FeatureSpace`\n# \n# To set up how each tabular feature should be preprocessed, `keras.utils.FeatureSpace` is used. A dictionary is passsed to it that maps each feature name to a string describing its type.\n# \n# - **String Categorical Features**: \n#   - Examples: `sex`, `anotm_site_general`\n# - **Numerical Features**:\n#   - Examples: `tbp_lv_nevi_confidence`, `clin_size_long_diam_mm`\n#   - Note: These features will be normalized.\n# - **Numerical Discrete Features**:\n#   - `age_approx`: Need to discretize this feature into a number of bins.\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020592,\"end_time\":\"2024-08-24T00:24:30.655103\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:30.634511\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Adapt Tabular Features\n# \n# Before the `FeatureSpace` is used to build a model, it needs to be adapted to the training data. During `adapt()`, the `FeatureSpace` will:\n# \n# - Index the set of possible values for **categorical features**.\n# - Compute the mean and variance for **numerical features** to normalize.\n# - Compute the value boundaries for the different bins for **numerical features** to discretize.\n# \n# Note: `adapt()` should be called on a `tf.data.Dataset` that yields dictionaries of feature values â€“ no labels.\n\n# %% [code] {\"papermill\":{\"duration\":110.613656,\"end_time\":\"2024-08-24T00:26:21.289648\",\"exception\":false,\"start_time\":\"2024-08-24T00:24:30.675992\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:24:04.029442Z\",\"iopub.execute_input\":\"2024-12-30T07:24:04.029791Z\",\"iopub.status.idle\":\"2024-12-30T07:25:06.976289Z\",\"shell.execute_reply.started\":\"2024-12-30T07:24:04.029754Z\",\"shell.execute_reply\":\"2024-12-30T07:25:06.975325Z\"}}\ntraining_ds_with_no_labels = training_ds.map(lambda x, _: x[\"features\"])\nfeature_space.adapt(training_ds_with_no_labels)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020907,\"end_time\":\"2024-08-24T00:26:21.334914\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:21.314007\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# At this point, the `FeatureSpace` can be called on a dictionary of raw feature values. It will return a single concatenated vector for each sample, combining encoded features.\n# \n# In the code below, it can be noticed that even though $12$ raw (tabular) features are used, after processing with `FeatureSpace`, a vector of size $71$ is created. This is because operations like one-hot encoding are being applied here.\n\n# %% [code] {\"papermill\":{\"duration\":2.193971,\"end_time\":\"2024-08-24T00:26:23.550022\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:21.356051\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:06.978592Z\",\"iopub.execute_input\":\"2024-12-30T07:25:06.978896Z\",\"iopub.status.idle\":\"2024-12-30T07:25:07.761070Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:06.978855Z\",\"shell.execute_reply\":\"2024-12-30T07:25:07.760141Z\"}}\nfor x, _ in training_ds.take(1):\n    preprocessed_x = feature_space(x[\"features\"])\n    print(\"preprocessed_x.shape:\", preprocessed_x.shape)\n    print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.021384,\"end_time\":\"2024-08-24T00:26:23.592643\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:23.571259\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Apply Feature Processing\n# \n# Integrating feature space processing into the data pipeline before the model is crucial. This approach enables asynchronous, parallel preprocessing of data on the CPU, ensuring it is optimized before being fed into the model.\n\n# %% [code] {\"papermill\":{\"duration\":0.303585,\"end_time\":\"2024-08-24T00:26:23.917493\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:23.613908\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:07.762606Z\",\"iopub.execute_input\":\"2024-12-30T07:25:07.763500Z\",\"iopub.status.idle\":\"2024-12-30T07:25:07.985618Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:07.763457Z\",\"shell.execute_reply\":\"2024-12-30T07:25:07.984941Z\"}}\ntraining_ds = training_ds.map(\n    lambda x, y: ({\"images\": x[\"images\"],\n                   \"features\": feature_space(x[\"features\"])}, y), num_parallel_calls=tf.data.AUTOTUNE)\n\nvalidation_ds = validation_ds.map(\n    lambda x, y: ({\"images\": x[\"images\"],\n                   \"features\": feature_space(x[\"features\"])}, y), num_parallel_calls=tf.data.AUTOTUNE)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020817,\"end_time\":\"2024-08-24T00:26:23.959558\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:23.938741\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Output Shape of a Batch\n# \n# Verifying the shape of a batch sample is essential. This step ensures that the dataloader is generating inputs with the correct dimensions, which is critical for the model's performance.\n\n# %% [code] {\"papermill\":{\"duration\":0.224381,\"end_time\":\"2024-08-24T00:26:24.205078\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:23.980697\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:07.986647Z\",\"iopub.execute_input\":\"2024-12-30T07:25:07.986951Z\",\"iopub.status.idle\":\"2024-12-30T07:25:08.082690Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:07.986913Z\",\"shell.execute_reply\":\"2024-12-30T07:25:08.081743Z\"}}\nbatch = next(iter(validation_ds))\n\nprint(\"Images:\",batch[0][\"images\"].shape)\nprint(\"Features:\", batch[0][\"features\"].shape)\nprint(\"Targets:\", batch[1].shape)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020799,\"end_time\":\"2024-08-24T00:26:24.247197\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:24.226398\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Dataset Check\n# \n# Visualizing samples along with their associated labels from the dataset is a vital step. This process helps in understanding the data distribution and ensures that the dataset is correctly labeled.\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020738,\"end_time\":\"2024-08-24T00:26:24.289039\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:24.268301\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ” | Loss & Metric\n# \n# This competition utilizes the Partial Area Under the Curve (pAUC) metric for evaluation. In Keras, a similar metric is ROC AUC, which can be used for approximate evaluation. For optimizing our model, binary cross entropy (BCE) loss from Keras will be used.\n# \n# It's important to note that BCE loss is sensitive to class imbalance. To mitigate this issue, the class weights will be used which is computed earlier. These weights will prioritize updating the model weights more strongly for the minority class (positive class), thereby mitigating class imbalance.\n\n# %% [code] {\"papermill\":{\"duration\":0.03872,\"end_time\":\"2024-08-24T00:26:24.348670\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:24.309950\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:08.084244Z\",\"iopub.execute_input\":\"2024-12-30T07:25:08.084874Z\",\"iopub.status.idle\":\"2024-12-30T07:25:08.096522Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:08.084827Z\",\"shell.execute_reply\":\"2024-12-30T07:25:08.095783Z\"}}\n# AUC\nauc = keras.metrics.AUC()\n\n# Loss\nloss = keras.losses.BinaryCrossentropy(label_smoothing=0.02)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.020733,\"end_time\":\"2024-08-24T00:26:24.390714\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:24.369981\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ¤– | Modeling\n# \n# In this notebook, we're using the `EfficientNetV2 B2` backbone from KerasCV's pretrained models to extract features from images. For processing tabular data, `Dense` layers are used. The final layer (head) is a `Dense` layer with a `sigmoid` activation function. The `sigmoid` activation is chosen because the target is binary; `softmax` would've been selected if the target was multiclass.\n# \n# To explore other backbones, one can simply modify the `preset` in the `CFG` (config). A list of available pretrained backbones can be found on the [KerasCV website](https://keras.io/api/keras_cv/models/).\n# \n# > **Note**: Since the size of tabular features likely to change due to feature space processing,  `feature_space.get_encoded_features()` is used to determine the final size of the tabular feature vector for building the model.\n\n# %% [code] {\"papermill\":{\"duration\":11.391541,\"end_time\":\"2024-08-24T00:26:35.803227\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:24.411686\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:33:23.732991Z\",\"iopub.execute_input\":\"2024-12-29T14:33:23.733314Z\",\"iopub.status.idle\":\"2024-12-29T14:33:31.059788Z\",\"shell.execute_reply.started\":\"2024-12-29T14:33:23.733287Z\",\"shell.execute_reply\":\"2024-12-29T14:33:31.058928Z\"}}\n# Define input layers\nimage_input = keras.Input(shape=(*CFG.image_size, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\":image_input, \"features\":feat_input}\n\n# Branch for image input\nbackbone = keras_cv.models.EfficientNetV2Backbone.from_preset(CFG.preset)\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.Dropout(0.2)(x1)\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(96, activation=\"selu\")(feat_input)\nx2 = keras.layers.Dense(128, activation=\"selu\")(x2)\nx2 = keras.layers.Dropout(0.1)(x2)\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Output layer\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(concat)\n\n# Build model\nmodel = keras.models.Model(inp, out)\n\n# Compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc],\n)\n\n# Model Summary\nmodel.summary()\n\n# %% [markdown] {\"papermill\":{\"duration\":0.026338,\"end_time\":\"2024-08-24T00:26:35.863410\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:35.837072\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Plot Model\n# \n# As our model is multi-input, it is difficult to understand what is going on inside the architecture. That is where `plot_model` from **Keras** can be very handy. Overall architecture is shown, making it easier to design or recheck our architecture.\n\n# %% [code] {\"_kg_hide-input\":false,\"papermill\":{\"duration\":0.287676,\"end_time\":\"2024-08-24T00:26:36.173049\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:35.885373\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:33:31.060855Z\",\"iopub.execute_input\":\"2024-12-29T14:33:31.061108Z\",\"iopub.status.idle\":\"2024-12-29T14:33:31.297298Z\",\"shell.execute_reply.started\":\"2024-12-29T14:33:31.061083Z\",\"shell.execute_reply\":\"2024-12-29T14:33:31.296470Z\"}}\nkeras.utils.plot_model(model, show_shapes=True, show_layer_names=True, dpi=60)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.024017,\"end_time\":\"2024-08-24T00:26:36.222948\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:36.198931\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # âš“ | LR Schedule\n# \n# A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.\n\n# %% [code] {\"_kg_hide-input\":true,\"papermill\":{\"duration\":0.037778,\"end_time\":\"2024-08-24T00:26:36.285399\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:36.247621\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:08.097561Z\",\"iopub.execute_input\":\"2024-12-30T07:25:08.097823Z\",\"iopub.status.idle\":\"2024-12-30T07:25:08.106085Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:08.097798Z\",\"shell.execute_reply\":\"2024-12-30T07:25:08.105249Z\"}}\nimport math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 2.5e-5, 5e-6 * batch_size, 0.8e-5\n    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback\n\n# %% [code] {\"_kg_hide-input\":true,\"papermill\":{\"duration\":3.396047,\"end_time\":\"2024-08-24T00:26:39.705391\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:36.309344\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:08.107177Z\",\"iopub.execute_input\":\"2024-12-30T07:25:08.107761Z\",\"iopub.status.idle\":\"2024-12-30T07:25:09.874805Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:08.107723Z\",\"shell.execute_reply\":\"2024-12-30T07:25:09.873681Z\"}}\ninputs, targets = next(iter(training_ds))\nimages = inputs[\"images\"]\nnum_images, NUMERIC_COLUMNS = 8, 4\n\nplt.figure(figsize=(4 * NUMERIC_COLUMNS, num_images // NUMERIC_COLUMNS * 4))\nfor i, (image, target) in enumerate(zip(images[:num_images], targets[:num_images])):\n    plt.subplot(num_images // NUMERIC_COLUMNS, NUMERIC_COLUMNS, i + 1)\n    image = image.numpy().astype(\"float32\")\n    target= target.numpy().astype(\"int32\")[0]\n    \n    image = (image - image.min()) / (image.max() + 1e-4)\n\n    plt.imshow(image)\n    plt.title(f\"Target: {target}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n# %% [code] {\"papermill\":{\"duration\":0.248004,\"end_time\":\"2024-08-24T00:26:39.996218\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:39.748214\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:09.877473Z\",\"iopub.execute_input\":\"2024-12-30T07:25:09.878078Z\",\"iopub.status.idle\":\"2024-12-30T07:25:10.112387Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:09.878039Z\",\"shell.execute_reply\":\"2024-12-30T07:25:10.111569Z\"}}\nlr_cb = get_lr_callback(CFG.batch_size, mode=\"exp\", plot=True)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.044576,\"end_time\":\"2024-08-24T00:26:40.084677\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:40.040101\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ’¾ | Model Checkpoint\n# \n# The `ModelCheckpoint` callback is used to save the model during training. It monitors the performance of the model on the validation data and saves the model with the best performance based on a specified metric.\n# \n# Following setup ensures that the model with the highest validation AUC (`val_auc`) during training is saved to the file `best_model.keras`. Only the best model is saved, preventing overwriting with worse-performing models.\n\n# %% [code] {\"papermill\":{\"duration\":0.052634,\"end_time\":\"2024-08-24T00:26:40.182576\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:40.129942\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:25:10.113708Z\",\"iopub.execute_input\":\"2024-12-30T07:25:10.114077Z\",\"iopub.status.idle\":\"2024-12-30T07:25:10.118717Z\",\"shell.execute_reply.started\":\"2024-12-30T07:25:10.114040Z\",\"shell.execute_reply\":\"2024-12-30T07:25:10.117962Z\"}}\nckpt_cb = keras.callbacks.ModelCheckpoint(\n    \"best_model.keras\",   # Filepath where the model will be saved.\n    monitor=\"val_auc\",    # Metric to monitor (validation AUC in this case).\n    save_best_only=True,  # Save only the model with the best performance.\n    save_weights_only=False,  # Save the entire model (not just the weights).\n    mode=\"max\",           # The model with the maximum 'val_auc' will be saved.\n)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.043497,\"end_time\":\"2024-08-24T00:26:40.269999\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:40.226502\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸš‚ | Training\n# \n# Follwing cell will train the prepared model. Notice that, `class_weight` is used in the training.\n\n# %% [code] {\"papermill\":{\"duration\":388.946941,\"end_time\":\"2024-08-24T00:33:09.261054\",\"exception\":false,\"start_time\":\"2024-08-24T00:26:40.314113\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:33:33.531652Z\",\"iopub.execute_input\":\"2024-12-29T14:33:33.531891Z\",\"iopub.status.idle\":\"2024-12-29T14:39:39.779428Z\",\"shell.execute_reply.started\":\"2024-12-29T14:33:33.531868Z\",\"shell.execute_reply\":\"2024-12-29T14:39:39.778709Z\"}}\nhistory = model.fit(\n    training_ds,\n    epochs=CFG.epochs,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.05758,\"end_time\":\"2024-08-24T00:33:09.376501\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:09.318921\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Visualize AUC with Epochs\n\n# %% [code] {\"_kg_hide-input\":true,\"papermill\":{\"duration\":0.438174,\"end_time\":\"2024-08-24T00:33:09.870545\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:09.432371\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:39:39.780984Z\",\"iopub.execute_input\":\"2024-12-29T14:39:39.781282Z\",\"iopub.status.idle\":\"2024-12-29T14:39:40.115930Z\",\"shell.execute_reply.started\":\"2024-12-29T14:39:39.781248Z\",\"shell.execute_reply\":\"2024-12-29T14:39:40.115087Z\"}}\n# Extract AUC and validation AUC from history\nauc = history.history['auc']\nval_auc = history.history['val_auc']\nepochs = range(1, len(auc) + 1)\n\n# Find the epoch with the maximum val_auc\nmax_val_auc_epoch = np.argmax(val_auc)\nmax_val_auc = val_auc[max_val_auc_epoch]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, auc, 'o-', label='Training AUC', markersize=5, color='tab:blue')\nplt.plot(epochs, val_auc, 's-', label='Validation AUC', markersize=5, color='tab:orange')\n\n# Highlight the max val_auc\nplt.scatter(max_val_auc_epoch + 1, max_val_auc, color='red', s=100, label=f'Max Val AUC: {max_val_auc:.4f}')\nplt.annotate(f'Max Val AUC: {max_val_auc:.4f}', \n             xy=(max_val_auc_epoch + 1, max_val_auc), \n             xytext=(max_val_auc_epoch + 1 + 0.5, max_val_auc - 0.05),\n             arrowprops=dict(facecolor='black', arrowstyle=\"->\"),\n             fontsize=12,\n             color='tab:red')\n\n# Enhancing the plot\nplt.title('AUC over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('AUC', fontsize=12)\nplt.legend(loc='lower right', fontsize=12)\nplt.grid(True)\nplt.xticks(epochs)\n\n# Show the plot\nplt.show()\n\n# %% [markdown] {\"papermill\":{\"duration\":0.057075,\"end_time\":\"2024-08-24T00:33:09.985784\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:09.928709\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ“‹ | Result\n\n# %% [code] {\"papermill\":{\"duration\":0.069471,\"end_time\":\"2024-08-24T00:33:10.113463\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:10.043992\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:39:40.117180Z\",\"iopub.execute_input\":\"2024-12-29T14:39:40.117589Z\",\"iopub.status.idle\":\"2024-12-29T14:39:40.124002Z\",\"shell.execute_reply.started\":\"2024-12-29T14:39:40.117548Z\",\"shell.execute_reply\":\"2024-12-29T14:39:40.123017Z\"}}\n# Best Result\nbest_score = max(history.history['val_auc'])\nbest_epoch = np.argmax(history.history['val_auc']) + 1\nprint(\"#\" * 10 + \" Result \" + \"#\" * 10)\nprint(f\"Best AUC: {best_score:.5f}\")\nprint(f\"Best Epoch: {best_epoch}\")\nprint(\"#\" * 28)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.058198,\"end_time\":\"2024-08-24T00:33:10.229522\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:10.171324\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# # ðŸ§ª | Prediction\n\n# %% [markdown] {\"papermill\":{\"duration\":0.05779,\"end_time\":\"2024-08-24T00:33:10.345958\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:10.288168\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Load Best Model\n\n# %% [code] {\"papermill\":{\"duration\":7.831032,\"end_time\":\"2024-08-24T00:33:18.233895\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:10.402863\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:39:40.124899Z\",\"iopub.execute_input\":\"2024-12-29T14:39:40.125107Z\",\"iopub.status.idle\":\"2024-12-29T14:39:47.358413Z\",\"shell.execute_reply.started\":\"2024-12-29T14:39:40.125086Z\",\"shell.execute_reply\":\"2024-12-29T14:39:47.357686Z\"}}\nmodel.load_weights(\"best_model.keras\")\n\n# %% [markdown] {\"papermill\":{\"duration\":0.05811,\"end_time\":\"2024-08-24T00:33:18.350427\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:18.292317\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Build Testing Dataset\n# \n# Don't forget to normalize for the testing data as well.\n\n# %% [code] {\"papermill\":{\"duration\":0.272864,\"end_time\":\"2024-08-24T00:33:18.682051\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:18.409187\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:39:47.359745Z\",\"iopub.execute_input\":\"2024-12-29T14:39:47.360042Z\",\"iopub.status.idle\":\"2024-12-29T14:39:47.528690Z\",\"shell.execute_reply.started\":\"2024-12-29T14:39:47.360015Z\",\"shell.execute_reply\":\"2024-12-29T14:39:47.528066Z\"}}\n# Testing\nprint(\"# Testing:\")\ntesting_features = dict(testing_df[FEAT_COLS])\ntesting_ids = testing_df.isic_id.values\ntesting_ds = build_dataset(testing_ids, testing_hdf5,\n                        testing_features, batch_size=CFG.batch_size,\n                         shuffle=False, augment=False, cache=False)\n# Apply feature space processing\ntesting_ds = testing_ds.map(\n    lambda x: {\"images\": x[\"images\"],\n               \"features\": feature_space(x[\"features\"])}, num_parallel_calls=tf.data.AUTOTUNE)\n\n# %% [markdown] {\"papermill\":{\"duration\":0.058486,\"end_time\":\"2024-08-24T00:33:18.799793\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:18.741307\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Inference\n\n# %% [code] {\"papermill\":{\"duration\":7.326659,\"end_time\":\"2024-08-24T00:33:26.185751\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:18.859092\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:39:47.529579Z\",\"iopub.execute_input\":\"2024-12-29T14:39:47.529790Z\",\"iopub.status.idle\":\"2024-12-29T14:39:53.647538Z\",\"shell.execute_reply.started\":\"2024-12-29T14:39:47.529768Z\",\"shell.execute_reply\":\"2024-12-29T14:39:53.646654Z\"}}\npreds = model.predict(testing_ds).squeeze()\n\n# %% [markdown] {\"papermill\":{\"duration\":0.058354,\"end_time\":\"2024-08-24T00:33:26.303388\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:26.245034\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false}}\n# ## Check Prediction\n\n# %% [code] {\"papermill\":{\"duration\":0.591497,\"end_time\":\"2024-08-24T00:33:26.952319\",\"exception\":false,\"start_time\":\"2024-08-24T00:33:26.360822\",\"status\":\"completed\"},\"tags\":[],\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T15:39:05.962410Z\",\"iopub.execute_input\":\"2024-12-29T15:39:05.962760Z\",\"iopub.status.idle\":\"2024-12-29T15:39:05.994658Z\",\"shell.execute_reply.started\":\"2024-12-29T15:39:05.962728Z\",\"shell.execute_reply\":\"2024-12-29T15:39:05.993532Z\"}}\ninputs = next(iter(testing_ds))\nimages = inputs[\"images\"]\n\n# Plotting\nplt.figure(figsize=(10, 4))\n\nfor i in range(3):\n    plt.subplot(1, 3, i+1)  # 1 row, 3 columns, i+1th subplot\n    plt.imshow(images[i])  # Show image\n    plt.title(f'Prediction: {preds[i]:.2f}')  # Set title with prediction\n    plt.axis('off')  # Hide axis\n\nplt.suptitle('Model Predictions on Testing Images', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## DenseNet121\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:39:54.169395Z\",\"iopub.execute_input\":\"2024-12-29T14:39:54.169662Z\",\"iopub.status.idle\":\"2024-12-29T14:48:44.603470Z\",\"shell.execute_reply.started\":\"2024-12-29T14:39:54.169636Z\",\"shell.execute_reply\":\"2024-12-29T14:48:44.602750Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy  # Import the necessary metrics\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\": image_input, \"features\": feat_input}\n\n# Branch for image input using EfficientNet backbone\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)  # Adding Batch Normalization\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)  # Increased units and changed activation\nx2 = keras.layers.Dense(256, activation=\"relu\")(x2)  # Increased units\nx2 = keras.layers.BatchNormalization()(x2)  # Adding Batch Normalization\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Output layer for binary classification (benign vs malignant)\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(concat)\n\n# Build the multi-modal model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n\n# Compile the model\nauc = AUC(name=\"auc\")\nprecision = Precision(name=\"precision\")  # Define precision metric\nrecall = Recall(name=\"recall\")  # Define recall metric\naccuracy = Accuracy(name=\"accuracy\")  # Define accuracy metric\nloss = keras.losses.BinaryCrossentropy(from_logits=False)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc, precision, recall, accuracy],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmodel.summary()\n\n# Training the model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:48:44.605325Z\",\"iopub.execute_input\":\"2024-12-29T14:48:44.605609Z\",\"iopub.status.idle\":\"2024-12-29T14:48:44.611161Z\",\"shell.execute_reply.started\":\"2024-12-29T14:48:44.605583Z\",\"shell.execute_reply\":\"2024-12-29T14:48:44.610359Z\"}}\n# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_accuracy = history.history['accuracy'][best_epoch]\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Accuracy: {best_accuracy:.5f}\")\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")\n\n# %% [markdown]\n# ## ResNet50\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:48:44.612191Z\",\"iopub.execute_input\":\"2024-12-29T14:48:44.612509Z\",\"iopub.status.idle\":\"2024-12-29T14:59:35.309644Z\",\"shell.execute_reply.started\":\"2024-12-29T14:48:44.612484Z\",\"shell.execute_reply\":\"2024-12-29T14:59:35.308788Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\": image_input, \"features\": feat_input}\n\n# Branch for image input using ResNet50\nbackbone = ResNet50(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.3)(x1)\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dense(64, activation=\"relu\")(x2)\nx2 = keras.layers.Dropout(0.2)(x2)\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Additional Dense layers after concatenation for deeper learning\nx = keras.layers.Dense(64, activation=\"relu\")(concat)\nx = keras.layers.BatchNormalization()(x)\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n\n# Build the multi-modal model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n\n# Compile the model with additional metrics (precision, recall, AUC)\nauc = AUC(name=\"auc\")\nprecision = Precision(name=\"precision\")\nrecall = Recall(name=\"recall\")\naccuracy = keras.metrics.BinaryAccuracy(name=\"accuracy\")\n\nloss = keras.losses.BinaryCrossentropy(from_logits=False)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc, precision, recall, accuracy],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmodel.summary()\n\n# Training the model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:59:35.311545Z\",\"iopub.execute_input\":\"2024-12-29T14:59:35.311819Z\",\"iopub.status.idle\":\"2024-12-29T14:59:35.317183Z\",\"shell.execute_reply.started\":\"2024-12-29T14:59:35.311792Z\",\"shell.execute_reply\":\"2024-12-29T14:59:35.316439Z\"}}\n# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_accuracy = history.history['accuracy'][best_epoch]\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Accuracy: {best_accuracy:.5f}\")\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")\n\n\n# %% [markdown]\n# ## InceptionV3\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T14:59:35.318336Z\",\"iopub.execute_input\":\"2024-12-29T14:59:35.318649Z\",\"iopub.status.idle\":\"2024-12-29T15:11:35.244637Z\",\"shell.execute_reply.started\":\"2024-12-29T14:59:35.318614Z\",\"shell.execute_reply\":\"2024-12-29T15:11:35.243923Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import InceptionV3\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\": image_input, \"features\": feat_input}\n\n# Branch for image input using InceptionV3\nbackbone = InceptionV3(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)  # Added BatchNormalization\nx1 = keras.layers.Dropout(0.3)(x1)  # Increased Dropout rate\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)  # Increased the first layer size\nx2 = keras.layers.BatchNormalization()(x2)  # Added BatchNormalization\nx2 = keras.layers.Dense(64, activation=\"relu\")(x2)  # Changed second layer size\nx2 = keras.layers.Dropout(0.2)(x2)  # Modified Dropout rate\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Additional Dense layers after concatenation for deeper learning\nx = keras.layers.Dense(64, activation=\"relu\")(concat)  # New dense layer\nx = keras.layers.BatchNormalization()(x)  # Added BatchNormalization\nx = keras.layers.Dropout(0.3)(x)  # Increased Dropout rate\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n\n# Build the multi-modal model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n\n# Compile the model\nauc = keras.metrics.AUC(name=\"auc\")\nloss = keras.losses.BinaryCrossentropy(from_logits=False)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc, precision, recall, accuracy],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmodel.summary()\n\n# Training the model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T15:11:35.246390Z\",\"iopub.execute_input\":\"2024-12-29T15:11:35.246663Z\",\"iopub.status.idle\":\"2024-12-29T15:11:35.251990Z\",\"shell.execute_reply.started\":\"2024-12-29T15:11:35.246635Z\",\"shell.execute_reply\":\"2024-12-29T15:11:35.251253Z\"}}\n# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_accuracy = history.history['accuracy'][best_epoch]\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Accuracy: {best_accuracy:.5f}\")\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")\n\n# %% [markdown]\n# ## Hierarchical Multi-Scale Attention Fusion Network \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T15:11:35.253347Z\",\"iopub.execute_input\":\"2024-12-29T15:11:35.253677Z\",\"iopub.status.idle\":\"2024-12-29T15:16:01.917560Z\",\"shell.execute_reply.started\":\"2024-12-29T15:11:35.253640Z\",\"shell.execute_reply\":\"2024-12-29T15:16:01.916808Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import regularizers\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# EfficientNet Backbone with Pretrained Weights\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.3)(x1)\n\n# First latent projection for image embeddings\nx1_latent = keras.layers.Dense(512, activation=\"relu\", name=\"image_latent_projection\")(x1)\n\n# ----------------- Feature Branch -----------------\n# Tabular Data Feature Processing with Increased Dense Layer Sizes\nx2 = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dropout(0.2)(x2)\n\n# First latent projection for feature embeddings\nx2_latent = keras.layers.Dense(512, activation=\"relu\", name=\"feature_latent_projection\")(x2)\n\n# ----------------- Latent Space Alignment -----------------\n# Discriminator for Alignment with Increased Network Capacity\ndef make_discriminator():\n    d_input = keras.Input(shape=(512,))\n    d_x = keras.layers.Dense(256, activation=\"relu\")(d_input)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(128, activation=\"relu\")(d_x)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(64, activation=\"relu\")(d_x)\n    d_output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"discriminator_output\")(d_x)\n    return keras.models.Model(inputs=d_input, outputs=d_output, name=\"Discriminator\")\n\ndiscriminator = make_discriminator()\n\n# Latent space alignment\nd_image = discriminator(x1_latent)\nd_feature = discriminator(x2_latent)\n\n# Loss for discriminator alignment\nadversarial_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n\n# ----------------- Feature Aggregation -----------------\n# Concatenate Latent Spaces\nconcat = keras.layers.Concatenate()([x1_latent, x2_latent])\n\n# Hierarchical Feature Aggregation with Larger Dense Layer\nagg = keras.layers.Dense(1024, activation=\"relu\", name=\"aggregated_features\")(concat)\nagg = keras.layers.BatchNormalization()(agg)\nagg = keras.layers.Dropout(0.3)(agg)\n\n# ----------------- Output Layer -----------------\n# Final Binary Classification\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"output\")(agg)\n\n# Build the MedBlendNet Model\nmedblendnet = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# Compile the model with a learning rate scheduler\nlr_scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n\nmedblendnet.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[auc, precision, recall, accuracy],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmedblendnet.summary()\n\n# Training the model with callbacks for better performance\nhistory = medblendnet.fit(\n    training_ds,\n    epochs=10,\n    callbacks=[lr_scheduler, ckpt_cb],  # Add callbacks as needed\n    validation_data=validation_ds,\n    verbose=10,\n    class_weight=class_weights,\n)\n\n\n\n# %% [code] {\"execution\":{\"execution_failed\":\"2024-12-29T15:29:52.452Z\"}}\n# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_accuracy = history.history['accuracy'][best_epoch]\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Accuracy: {best_accuracy:.5f}\")\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T06:16:25.291398Z\",\"iopub.execute_input\":\"2024-12-30T06:16:25.291765Z\",\"iopub.status.idle\":\"2024-12-30T06:23:18.159635Z\",\"shell.execute_reply.started\":\"2024-12-30T06:16:25.291732Z\",\"shell.execute_reply\":\"2024-12-30T06:23:18.158895Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import AUC, Precision, Recall, Accuracy\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# EfficientNet Backbone with Pretrained Weights\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.5)(x1)  # Increased dropout in the image branch\n\n# First latent projection for image embeddings\nx1_latent = keras.layers.Dense(512, activation=\"relu\", name=\"image_latent_projection\")(x1)\nx1_latent = keras.layers.BatchNormalization()(x1_latent)\n\n# ----------------- Feature Branch -----------------\n# Tabular Data Feature Processing with Increased Dense Layer Sizes\nx2 = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dropout(0.4)(x2)  # Increased dropout in the feature branch\n\n# First latent projection for feature embeddings\nx2_latent = keras.layers.Dense(512, activation=\"relu\", name=\"feature_latent_projection\")(x2)\nx2_latent = keras.layers.BatchNormalization()(x2_latent)\n\n# ----------------- Latent Space Alignment -----------------\n# Discriminator for Alignment with Increased Network Capacity\ndef make_discriminator():\n    d_input = keras.Input(shape=(512,))\n    d_x = keras.layers.Dense(256, activation=\"relu\")(d_input)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(128, activation=\"relu\")(d_x)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(64, activation=\"relu\")(d_x)\n    d_output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"discriminator_output\")(d_x)\n    return keras.models.Model(inputs=d_input, outputs=d_output, name=\"Discriminator\")\n\ndiscriminator = make_discriminator()\n\n# Latent space alignment\nd_image = discriminator(x1_latent)\nd_feature = discriminator(x2_latent)\n\n# Loss for discriminator alignment\nadversarial_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n\n# ----------------- Feature Aggregation -----------------\n# Concatenate Latent Spaces\nconcat = keras.layers.Concatenate()([x1_latent, x2_latent])\n\n# Hierarchical Feature Aggregation with Larger Dense Layer\nagg = keras.layers.Dense(2048, activation=\"relu\", name=\"aggregated_features\")(concat)\nagg = keras.layers.BatchNormalization()(agg)\nagg = keras.layers.Dropout(0.5)(agg)  # Increased dropout\n\n# ----------------- Output Layer -----------------\n# Final Binary Classification\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"output\")(agg)\n\n# Build the MedBlendNet Model\nmedblendnet = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# ----------------- Compile the Model -----------------\n# Early stopping and learning rate scheduler for better performance\nearly_stopping_cb = EarlyStopping(monitor=\"val_auc\", patience=10, restore_best_weights=True, verbose=1, mode='max')\nlr_scheduler = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, verbose=1, mode='max')\n\n# Compile the model\nmedblendnet.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[AUC(), Precision(), Recall(), Accuracy()],\n)\n\n# Model summary\nmedblendnet.summary()\n\n# ----------------- Train the Model -----------------\nhistory = medblendnet.fit(\n    training_ds,\n    epochs=20,\n    callbacks=[lr_scheduler, early_stopping_cb, ckpt_cb],  # Add checkpoints if needed\n    validation_data=validation_ds,\n    verbose=1,\n    class_weight=class_weights,\n)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T06:23:55.504030Z\",\"iopub.execute_input\":\"2024-12-30T06:23:55.504375Z\",\"iopub.status.idle\":\"2024-12-30T06:23:55.509624Z\",\"shell.execute_reply.started\":\"2024-12-30T06:23:55.504344Z\",\"shell.execute_reply\":\"2024-12-30T06:23:55.508672Z\"}}\n# Step 1: Find the epoch with the best validation AUC\nbest_epoch = history.history['auc_'].index(max(history.history['val_auc_3']))  # Index of the max AUC\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_val_auc = history.history['val_auc_3'][best_epoch]\n\n\nprint(f\"Best AUC: {best_val_auc:.5f}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T16:52:09.222849Z\",\"iopub.execute_input\":\"2024-12-29T16:52:09.223212Z\",\"iopub.status.idle\":\"2024-12-29T16:52:09.228250Z\",\"shell.execute_reply.started\":\"2024-12-29T16:52:09.223178Z\",\"shell.execute_reply\":\"2024-12-29T16:52:09.227165Z\"}}\nprint(history.history.keys())\n\n\n# %% [markdown]\n# ## Updated one\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:59:41.134413Z\",\"iopub.execute_input\":\"2024-12-30T07:59:41.135100Z\",\"iopub.status.idle\":\"2024-12-30T08:07:11.470720Z\",\"shell.execute_reply.started\":\"2024-12-30T07:59:41.135066Z\",\"shell.execute_reply\":\"2024-12-30T08:07:11.469785Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import AUC, Precision, Recall, Accuracy\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# EfficientNet Backbone with Pretrained Weights\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.5)(x1)  # Increased dropout in the image branch\n\n# First latent projection for image embeddings\nx1_latent = keras.layers.Dense(512, activation=\"relu\", name=\"image_latent_projection\")(x1)\nx1_latent = keras.layers.BatchNormalization()(x1_latent)\n\n# ----------------- Feature Branch -----------------\n# Tabular Data Feature Processing with Increased Dense Layer Sizes\nx2 = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dropout(0.4)(x2)  # Increased dropout in the feature branch\n\n# First latent projection for feature embeddings\nx2_latent = keras.layers.Dense(512, activation=\"relu\", name=\"feature_latent_projection\")(x2)\nx2_latent = keras.layers.BatchNormalization()(x2_latent)\n\n# ----------------- Latent Space Alignment -----------------\n# Discriminator for Alignment with Increased Network Capacity\ndef make_discriminator():\n    d_input = keras.Input(shape=(512,))\n    d_x = keras.layers.Dense(256, activation=\"relu\")(d_input)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(128, activation=\"relu\")(d_x)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(64, activation=\"relu\")(d_x)\n    d_output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"discriminator_output\")(d_x)\n    return keras.models.Model(inputs=d_input, outputs=d_output, name=\"Discriminator\")\n\ndiscriminator = make_discriminator()\n\n# Latent space alignment\nd_image = discriminator(x1_latent)\nd_feature = discriminator(x2_latent)\n\n# Loss for discriminator alignment\nadversarial_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n\n# ----------------- Feature Aggregation -----------------\n# Concatenate Latent Spaces\nconcat = keras.layers.Concatenate()([x1_latent, x2_latent])\n\n# Hierarchical Feature Aggregation with Larger Dense Layer\nagg = keras.layers.Dense(2048, activation=\"relu\", name=\"aggregated_features\")(concat)\nagg = keras.layers.BatchNormalization()(agg)\nagg = keras.layers.Dropout(0.5)(agg)  # Increased dropout\n\n# ----------------- Output Layer -----------------\n# Final Binary Classification\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"output\")(agg)\n\n# Build the MedBlendNet Model\nmedblendnet = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# ----------------- Compile the Model -----------------\n# Early stopping and learning rate scheduler for better performance\nearly_stopping_cb = EarlyStopping(monitor=\"val_auc\", patience=10, restore_best_weights=True, verbose=1, mode='max')\nlr_scheduler = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, verbose=1, mode='max')\n\n# Compile the model\nmedblendnet.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[AUC(), Precision(), Recall(), Accuracy()],\n)\n\n# Model summary\nmedblendnet.summary()\n\n# ----------------- Train the Model -----------------\nhistory = medblendnet.fit(\n    training_ds,\n    epochs=20,\n    callbacks=[lr_scheduler, early_stopping_cb, ckpt_cb],  # Add checkpoints if needed\n    validation_data=validation_ds,\n    verbose=1,\n    class_weight=class_weights,\n)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T08:10:00.183910Z\",\"iopub.execute_input\":\"2024-12-30T08:10:00.184256Z\",\"iopub.status.idle\":\"2024-12-30T08:10:00.189493Z\",\"shell.execute_reply.started\":\"2024-12-30T08:10:00.184226Z\",\"shell.execute_reply\":\"2024-12-30T08:10:00.188604Z\"}}\n# Get the best AUC validation score\nbest_val_auc_epoch = history.history['val_auc_1'].index(max(history.history['val_auc_1']))\nbest_val_auc = history.history['val_auc_1'][best_val_auc_epoch]\n\n# Output the results\nprint(f\"Best AUC-Validation: {best_val_auc:.5f} at epoch {best_val_auc_epoch+1}\")\n\n# %% [markdown]\n# ## CMMANN \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T06:34:16.835226Z\",\"iopub.status.idle\":\"2024-12-30T06:34:16.835555Z\",\"shell.execute_reply.started\":\"2024-12-30T06:34:16.835392Z\",\"shell.execute_reply\":\"2024-12-30T06:34:16.835407Z\"},\"_kg_hide-input\":true}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB4  # Upgraded from B0\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nclass MemoryAugmentedNetwork(keras.layers.Layer):\n    def __init__(self, memory_size=512, memory_dim=256, temperature=0.1, **kwargs):  # Increased memory capacity\n        super(MemoryAugmentedNetwork, self).__init__(**kwargs)\n        self.memory_size = memory_size\n        self.memory_dim = memory_dim\n        self.temperature = temperature\n        self.memory = self.add_weight(\n            name=\"memory\",\n            shape=(memory_size, memory_dim),\n            initializer=\"glorot_uniform\",  # Changed initializer\n            trainable=True,\n        )\n        \n    def call(self, inputs):\n        # Scaled dot-product attention with temperature\n        similarity = tf.matmul(inputs, self.memory, transpose_b=True) / self.temperature\n        attention_weights = tf.nn.softmax(similarity, axis=-1)\n        memory_read = tf.matmul(attention_weights, self.memory)\n        # Residual connection\n        return memory_read + inputs\n\n# Image and Tabular Data Inputs\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(71,), name=\"features\")\n# ----------------- Image Branch -----------------\n# Using EfficientNetB4 for better feature extraction\nbackbone = EfficientNetB4(weights=None, include_top=False, input_shape=(128, 128, 3))\n# Freeze early layers\nfor layer in backbone.layers[:100]:\n    layer.trainable = False\n\nx1 = backbone(image_input)\nx1 = layers.GlobalAveragePooling2D()(x1)\nx1 = layers.BatchNormalization()(x1)\nx1 = layers.Dense(512, activation=\"relu\")(x1)  # Additional dense layer\nx1 = layers.Dropout(0.3)(x1)  # Increased dropout\n\n# Latent Representation for Image Features\nx1_latent = layers.Dense(512, activation=\"relu\", \n                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n                        name=\"image_latent_projection\")(x1)\n\n# ----------------- Feature Branch -----------------\n# Enhanced Tabular Data Processing\nx2 = layers.Dense(256, activation=\"relu\")(feat_input)\nx2 = layers.BatchNormalization()(x2)\nx2 = layers.Dense(512, activation=\"relu\")(x2)\nx2 = layers.BatchNormalization()(x2)\nx2 = layers.Dropout(0.3)(x2)\n\n# Latent Representation for Tabular Features\nx2_latent = layers.Dense(512, activation=\"relu\",\n                        kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n                        name=\"feature_latent_projection\")(x2)\n\n# ----------------- Memory-Augmented Module -----------------\nmemory_module = MemoryAugmentedNetwork(memory_size=512, memory_dim=256, temperature=0.1)\nx1_latent_projected = layers.Dense(256, activation=\"relu\")(x1_latent)\nx2_latent_projected = layers.Dense(256, activation=\"relu\")(x2_latent)\n\nx1_mem = memory_module(x1_latent_projected)\nx2_mem = memory_module(x2_latent_projected)\n\n# ----------------- Enhanced Contrastive Loss -----------------\ndef improved_contrastive_loss(y_true, y_pred, margin=2.0, alpha=0.4):\n    # Binary cross-entropy component\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    \n    # Enhanced contrastive component\n    pos_pair_distance = tf.reduce_sum(tf.square(y_pred - y_true), axis=-1)\n    neg_pair_distance = tf.maximum(margin - pos_pair_distance, 0.0)\n    \n    # Focal loss component\n    gamma = 2.0\n    focal_weight = tf.pow(1. - y_pred, gamma) * y_true + tf.pow(y_pred, gamma) * (1. - y_true)\n    focal = focal_weight * bce\n    \n    return alpha * (pos_pair_distance + neg_pair_distance) + (1 - alpha) * focal\n\n# ----------------- Feature Aggregation -----------------\nconcat = layers.Concatenate(axis=-1)([x1_mem, x2_mem])\n# Enhanced Cross-Modal Feature Aggregation\nagg = layers.Dense(1024, activation=\"relu\", \n                  kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(concat)\nagg = layers.BatchNormalization()(agg)\nagg = layers.Dropout(0.4)(agg)\nagg = layers.Dense(512, activation=\"relu\")(agg)\nagg = layers.BatchNormalization()(agg)\nagg = layers.Dropout(0.3)(agg)\n# ----------------- Output Layer -----------------\nout = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(agg)\n# Build Model\ncmmann = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n# Compile with custom metrics\ncmmann.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=5e-5),  # Lower initial learning rate\n    loss=improved_contrastive_loss,\n    metrics=[\n        tf.keras.metrics.AUC(name='auc'),\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n    ]\n)\n# ----------------- Enhanced Callbacks -----------------\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_auc',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-6,\n    mode='max'\n)\nearly_stopping = EarlyStopping(\n    monitor='val_auc',\n    patience=15,\n    restore_best_weights=True,\n    mode='max'\n)\n# ----------------- Enhanced Data Augmentation -----------------\ntrain_datagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest',\n    brightness_range=[0.7, 1.3],\n    channel_shift_range=50.0\n)\n# ----------------- Training -----------------\nhistory = cmmann.fit(\n    training_ds,\n    epochs=10,  # Increased epochs\n    callbacks=[early_stopping, reduce_lr],\n    validation_data=validation_ds,\n    verbose=1,\n    class_weight=class_weights\n)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-29T16:26:41.636702Z\",\"iopub.execute_input\":\"2024-12-29T16:26:41.637459Z\",\"iopub.status.idle\":\"2024-12-29T16:26:41.643258Z\",\"shell.execute_reply.started\":\"2024-12-29T16:26:41.637425Z\",\"shell.execute_reply\":\"2024-12-29T16:26:41.642417Z\"}}\n# Step 1: Find the epoch with the best validation AUC\nbest_epoch = history.history['val_auc'].index(max(history.history['val_auc']))  # Find the index of the best AUC\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\nbest_accuracy = history.history['accuracy'][best_epoch]\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Accuracy: {best_accuracy:.5f}\")\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T06:34:21.831487Z\",\"iopub.execute_input\":\"2024-12-30T06:34:21.831851Z\",\"iopub.status.idle\":\"2024-12-30T06:47:12.517699Z\",\"shell.execute_reply.started\":\"2024-12-30T06:34:21.831821Z\",\"shell.execute_reply\":\"2024-12-30T06:47:12.516689Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\n# Memory-Augmented Network\nclass MemoryAugmentedNetwork(keras.layers.Layer):\n    def __init__(self, memory_size=512, memory_dim=256, temperature=0.1, **kwargs):\n        super(MemoryAugmentedNetwork, self).__init__(**kwargs)\n        self.memory_size = memory_size\n        self.memory_dim = memory_dim\n        self.temperature = temperature\n        self.memory = self.add_weight(\n            name=\"memory\",\n            shape=(memory_size, memory_dim),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n        )\n        \n    def call(self, inputs):\n        # Scaled dot-product attention with temperature\n        similarity = tf.matmul(inputs, self.memory, transpose_b=True) / self.temperature\n        attention_weights = tf.nn.softmax(similarity, axis=-1)\n        memory_read = tf.matmul(attention_weights, self.memory)\n        # Residual connection\n        return memory_read + inputs\n\n# Image and Tabular Data Inputs\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(71,), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# Using EfficientNetB4 for better feature extraction without pre-trained weights\nbackbone = EfficientNetB4(weights=None, include_top=False, input_shape=(128, 128, 3))\n\n# Freeze early layers\nfor layer in backbone.layers[:100]:\n    layer.trainable = False\n\nx1 = backbone(image_input)\nx1 = layers.GlobalAveragePooling2D()(x1)\nx1 = layers.BatchNormalization()(x1)\nx1 = layers.Dense(512, activation=\"relu\")(x1)\nx1 = layers.Dropout(0.5)(x1)  # Increased dropout\n\n# Latent Representation for Image Features\nx1_latent = layers.Dense(512, activation=\"relu\", \n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name=\"image_latent_projection\")(x1)\n\n# ----------------- Feature Branch -----------------\n# Enhanced Tabular Data Processing\nx2 = layers.Dense(256, activation=\"relu\")(feat_input)\nx2 = layers.BatchNormalization()(x2)\nx2 = layers.Dense(512, activation=\"relu\")(x2)\nx2 = layers.BatchNormalization()(x2)\nx2 = layers.Dropout(0.5)(x2)  # Increased dropout\n\n# Latent Representation for Tabular Features\nx2_latent = layers.Dense(512, activation=\"relu\",\n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name=\"feature_latent_projection\")(x2)\n\n# ----------------- Memory-Augmented Module -----------------\nmemory_module = MemoryAugmentedNetwork(memory_size=512, memory_dim=256, temperature=0.1)\nx1_latent_projected = layers.Dense(256, activation=\"relu\")(x1_latent)\nx2_latent_projected = layers.Dense(256, activation=\"relu\")(x2_latent)\n\nx1_mem = memory_module(x1_latent_projected)\nx2_mem = memory_module(x2_latent_projected)\n\n# ----------------- Enhanced Contrastive Loss -----------------\ndef improved_contrastive_loss(y_true, y_pred, margin=1.5, alpha=0.6):\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    \n    # Enhanced contrastive component\n    pos_pair_distance = tf.reduce_sum(tf.square(y_pred - y_true), axis=-1)\n    neg_pair_distance = tf.maximum(margin - pos_pair_distance, 0.0)\n    \n    # Focal loss component\n    gamma = 2.0\n    focal_weight = tf.pow(1. - y_pred, gamma) * y_true + tf.pow(y_pred, gamma) * (1. - y_true)\n    focal = focal_weight * bce\n    \n    return alpha * (pos_pair_distance + neg_pair_distance) + (1 - alpha) * focal\n\n# ----------------- Feature Aggregation -----------------\nconcat = layers.Concatenate(axis=-1)([x1_mem, x2_mem])\n\n# Enhanced Cross-Modal Feature Aggregation\nagg = layers.Dense(1024, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(concat)\nagg = layers.BatchNormalization()(agg)\nagg = layers.Dropout(0.5)(agg)  # Increased dropout\nagg = layers.Dense(512, activation=\"relu\")(agg)\nagg = layers.BatchNormalization()(agg)\nagg = layers.Dropout(0.5)(agg)  # Increased dropout\n\n# ----------------- Output Layer -----------------\nout = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(agg)\n\n# Build Model\ncmmann = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# Compile with custom metrics\ncmmann.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n    loss=improved_contrastive_loss,\n    metrics=[\n        tf.keras.metrics.AUC(name='auc'),\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n    ]\n)\n\n# ----------------- Enhanced Callbacks -----------------\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_auc',\n    factor=0.4,  # Lower factor\n    patience=6,  # Adjusted patience\n    min_lr=1e-6,\n    mode='max'\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_auc',\n    patience=20,  # Increased patience\n    restore_best_weights=True,\n    mode='max'\n)\n\n# ----------------- Enhanced Data Augmentation -----------------\ntrain_datagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest',\n    brightness_range=[0.7, 1.3],\n    channel_shift_range=50.0\n)\n\n# ----------------- Training -----------------\nhistory = cmmann.fit(\n    training_ds,\n    epochs=20,  # Increased epochs\n    callbacks=[early_stopping, reduce_lr],\n    validation_data=validation_ds,\n    verbose=1,\n    class_weight=class_weights\n)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:54:49.717285Z\",\"iopub.execute_input\":\"2024-12-30T07:54:49.718429Z\",\"iopub.status.idle\":\"2024-12-30T07:54:49.724179Z\",\"shell.execute_reply.started\":\"2024-12-30T07:54:49.718382Z\",\"shell.execute_reply\":\"2024-12-30T07:54:49.723271Z\"}}\n# Get the best AUC validation score\nbest_val_auc_epoch = history.history['val_auc'].index(max(history.history['val_auc']))\nbest_val_auc = history.history['val_auc'][best_val_auc_epoch]\n\n# Output the results\nprint(f\"Best AUC-Validation: {best_val_auc:.5f} at epoch {best_val_auc_epoch+1}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T08:15:51.602731Z\",\"iopub.execute_input\":\"2024-12-30T08:15:51.603102Z\",\"iopub.status.idle\":\"2024-12-30T08:23:34.547086Z\",\"shell.execute_reply.started\":\"2024-12-30T08:15:51.603053Z\",\"shell.execute_reply\":\"2024-12-30T08:23:34.546126Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import AUC, Precision, Recall, Accuracy\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# EfficientNet Backbone with Pretrained Weights\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\n\n# Add Spatial Attention to Image Branch\n# 1. Convolutional layer for attention map (before flattening)\nattention_map = keras.layers.Conv2D(1, (1, 1), activation='sigmoid', name=\"spatial_attention\")(x1)\nattention_map = keras.layers.UpSampling2D(size=(x1.shape[1] // attention_map.shape[1], x1.shape[2] // attention_map.shape[2]), interpolation=\"nearest\")(attention_map)  # Upsample to match x1\nx1_attention = keras.layers.Multiply()([x1, attention_map])  # Apply attention map\n\n# Flatten the output\nx1 = keras.layers.GlobalAveragePooling2D()(x1_attention)\n\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.5)(x1)  # Increased dropout in the image branch\n\n# First latent projection for image embeddings\nx1_latent = keras.layers.Dense(512, activation=\"relu\", name=\"image_latent_projection\")(x1)\nx1_latent = keras.layers.BatchNormalization()(x1_latent)\n\n# ----------------- Feature Branch -----------------\n# Tabular Data Feature Processing with Increased Dense Layer Sizes\nx2 = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dropout(0.4)(x2)  # Increased dropout in the feature branch\n\n# Add Feature Attention (using a simple attention mechanism on the feature branch)\nattention_weights = keras.layers.Dense(1, activation='softmax')(x2)  # Attention weights for features\nx2_attention = keras.layers.Multiply()([x2, attention_weights])  # Apply feature attention\n\n# First latent projection for feature embeddings\nx2_latent = keras.layers.Dense(512, activation=\"relu\", name=\"feature_latent_projection\")(x2_attention)\nx2_latent = keras.layers.BatchNormalization()(x2_latent)\n\n# ----------------- Latent Space Alignment -----------------\n# Discriminator for Alignment with Increased Network Capacity\ndef make_discriminator():\n    d_input = keras.Input(shape=(512,))\n    d_x = keras.layers.Dense(256, activation=\"relu\")(d_input)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(128, activation=\"relu\")(d_x)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(64, activation=\"relu\")(d_x)\n    d_output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"discriminator_output\")(d_x)\n    return keras.models.Model(inputs=d_input, outputs=d_output, name=\"Discriminator\")\n\ndiscriminator = make_discriminator()\n\n# Latent space alignment\nd_image = discriminator(x1_latent)\nd_feature = discriminator(x2_latent)\n\n# Loss for discriminator alignment\nadversarial_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n\n# ----------------- Feature Aggregation -----------------\n# Concatenate Latent Spaces\nconcat = keras.layers.Concatenate()([x1_latent, x2_latent])\n\n# Hierarchical Feature Aggregation with Larger Dense Layer\nagg = keras.layers.Dense(2048, activation=\"relu\", name=\"aggregated_features\")(concat)\nagg = keras.layers.BatchNormalization()(agg)\nagg = keras.layers.Dropout(0.5)(agg)  # Increased dropout\n\n# ----------------- Output Layer -----------------\n# Final Binary Classification\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"output\")(agg)\n\n# Build the MedBlendNet Model\nmedblendnet = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# ----------------- Compile the Model -----------------\n# Early stopping and learning rate scheduler for better performance\nearly_stopping_cb = EarlyStopping(monitor=\"val_auc\", patience=10, restore_best_weights=True, verbose=1, mode='max')\nlr_scheduler = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, verbose=1, mode='max')\n\n# Compile the model\nmedblendnet.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[AUC(), Precision(), Recall(), Accuracy()],\n)\n\n# Model summary\nmedblendnet.summary()\n\n# ----------------- Train the Model -----------------\nhistory = medblendnet.fit(\n    training_ds,\n    epochs=20,\n    callbacks=[lr_scheduler, early_stopping_cb, ckpt_cb],  # Add checkpoints if needed\n    validation_data=validation_ds,\n    verbose=1,\n    class_weight=class_weights,\n)\n\n\n# %% [code]\n# Get the best AUC validation score\nbest_val_auc_epoch = history.history['val_auc'].index(max(history.history['val_auc']))\nbest_val_auc = history.history['val_auc'][best_val_auc_epoch]\n\n# Output the results\nprint(f\"Best AUC-Validation: {best_val_auc:.5f} at epoch {best_val_auc_epoch+1}\")\n\n# %% [markdown]\n# ## Attention Model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:38:01.900050Z\",\"iopub.execute_input\":\"2024-12-30T07:38:01.900389Z\",\"iopub.status.idle\":\"2024-12-30T07:45:29.106079Z\",\"shell.execute_reply.started\":\"2024-12-30T07:38:01.900362Z\",\"shell.execute_reply\":\"2024-12-30T07:45:29.105141Z\"}}\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Attention, Add, MultiHeadAttention, Dense, Flatten, BatchNormalization, Concatenate, Dropout, GlobalAveragePooling2D, Reshape\nfrom tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy\nfrom tensorflow.keras import regularizers\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# EfficientNetB0 Backbone with Batch Normalization\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = GlobalAveragePooling2D()(x1)\nx1 = BatchNormalization()(x1)  # Adding Batch Normalization\nx1 = Dropout(0.3)(x1)  # Dropout to prevent overfitting\n\n# Attention Mechanism\nx1_reshaped = Reshape((1, 1280))(x1)\nx1_attention = MultiHeadAttention(num_heads=8, key_dim=128)(x1_reshaped, x1_reshaped)\nx1 = Add()([x1_reshaped, x1_attention])\nx1 = BatchNormalization()(x1)  # Batch normalization after attention\nx1 = Flatten()(x1)\n\n# Tabular Branch with DenseNet-like connections and regularization\nx2 = Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(feat_input)\nx2 = Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x2)\nx2 = BatchNormalization()(x2)\nx2 = Dropout(0.3)(x2)  # Dropout for regularization\n\n# Attention mechanism for tabular features\nx2_reshaped = Reshape((1, 256))(x2)\nx2_attention = MultiHeadAttention(num_heads=8, key_dim=128)(x2_reshaped, x2_reshaped)\nx2 = Add()([x2_reshaped, x2_attention])\nx2 = BatchNormalization()(x2)  # Batch normalization after attention\nx2 = Flatten()(x2)\n\n# Combine Image and Tabular Branches\nconcat = Concatenate()([x1, x2])\n\n# Output Layer\nout = Dense(1, activation=\"sigmoid\")(concat)\n\n# Build Model\nmodel = keras.models.Model(inputs=[image_input, feat_input], outputs=out)\n\n# Compile the model\nauc = AUC(name=\"auc\")\nprecision = Precision(name=\"precision\")\nrecall = Recall(name=\"recall\")\naccuracy = Accuracy(name=\"accuracy\")\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss='binary_crossentropy',\n    metrics=[auc, precision, recall, accuracy],\n)\n\n# Model Summary\nmodel.summary()\n\n# Fit the Model\nhistory = model.fit(\n    training_ds,\n    epochs=20,\n    validation_data=validation_ds,\n    callbacks=[lr_cb, ckpt_cb],\n    class_weight=class_weights,\n    verbose=CFG.verbose,\n)\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-30T07:52:06.370054Z\",\"iopub.execute_input\":\"2024-12-30T07:52:06.370400Z\",\"iopub.status.idle\":\"2024-12-30T07:52:06.375675Z\",\"shell.execute_reply.started\":\"2024-12-30T07:52:06.370372Z\",\"shell.execute_reply\":\"2024-12-30T07:52:06.374757Z\"}}\n# Get the best AUC validation score\nbest_val_auc_epoch = history.history['val_auc'].index(max(history.history['val_auc']))\nbest_val_auc = history.history['val_auc'][best_val_auc_epoch]\n\n# Output the results\nprint(f\"Best AUC-Validation: {best_val_auc:.5f} at epoch {best_val_auc_epoch+1}\")\n","metadata":{"_uuid":"d4dc4885-d44b-457d-824e-729137834531","_cell_guid":"602b4cd7-f057-4ac2-9dea-2b5675276a95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}